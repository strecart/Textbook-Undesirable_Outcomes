### 16. Machine Learning models for prediction of 60-month mortality

import numpy as np
import pandas as pd

import sklearn
import sklearn.metrics
from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, average_precision_score, auc, precision_score, recall_score, accuracy_score, balanced_accuracy_score, brier_score_loss, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from math import sqrt
from scipy import stats as st
from random import randrange

from matplotlib import pyplot
import seaborn as sns

from catboost import CatBoostClassifier
import xgboost as xgb
import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

import optuna
from optuna.samplers import TPESampler

import shap
from sklearn.inspection import PartialDependenceDisplay

#Define data and variables

data = pd.read_csv('ncdbto_preprocessed.csv')

variables = ['AGE', 'SEX', 'RACE', 'RX_SUMM_SURG_PRIM_SITE', 'RX_HOSP_SURG_APPR_2010', 'CDCC_TOTAL_BEST', 'UR_CD_13',
             'INSURANCE_STATUS','MED_INC_QUAR_2020', 'FACILITY_TYPE_CD', 'PRIMARY_SITE', 'HISTOLOGY', 'GRADE_MERGED',
             'TNM_CLIN_T_FINAL', 'TNM_CLIN_N_FINAL', 'TNM_CLIN_M_FINAL', 'REGIONAL_NODES_EXAMINED',
             'RX_SUMM_SYSTEMIC_SUR_SEQ', 'DX_LASTCONTACT_DEATH_MONTHS', 'PUF_VITAL_STATUS']

data = data[variables]

data.loc[(data['PUF_VITAL_STATUS'] == 'Dead') & (data['DX_LASTCONTACT_DEATH_MONTHS'] < 60), 'OUTCOME'] = 1
data.loc[(data['PUF_VITAL_STATUS'] == 'Dead') & (data['DX_LASTCONTACT_DEATH_MONTHS'] >= 60), 'OUTCOME'] = 0
data.loc[(data['PUF_VITAL_STATUS'] == 'Alive') & (data['DX_LASTCONTACT_DEATH_MONTHS'] >= 60), 'OUTCOME'] = 0
data = data.dropna(subset=['OUTCOME'])

data['OUTCOME'].value_counts(normalize=False, dropna=False)

### define predictors and outcome (x and y respectively)

outcomes = ['PUF_VITAL_STATUS', 'DX_LASTCONTACT_DEATH_MONTHS', 'OUTCOME']

x = data.drop(outcomes, axis = 1)
y = data['OUTCOME']


### Split into training, validation and testing datasets- first training + validation and testing

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)


print("Number patients x_train dataset: ", x_train.shape[0])
print("Number patients y_train dataset: ", y_train.shape[0])
print("Number patients x_test dataset: ", x_test.shape[0])
print("Number patients y_test dataset: ", y_test.shape[0])

# split into training and validation

train_x, valid_x, train_y, valid_y = train_test_split(x_train, y_train, test_size = 0.25, random_state = 0)

print("Number patients train_x dataset: ", train_x.shape[0])
print("Number patients train_y dataset: ", train_y.shape[0])
print("Number patients valid_x dataset: ", valid_x.shape[0])
print("Number patients valid_y dataset: ", valid_y.shape[0])

#Describe outcome of interest before resampling.

print("Before resampling, counts of label '1': {}".format(sum(train_y == 1)))
print("Before resampling, counts of label '0': {} \n".format(sum(train_y == 0)))

### SMOTE for class imbalance with checks of dataframe

from imblearn.over_sampling import SMOTE

resampler = SMOTE(random_state = 0)
train_x, train_y = resampler.fit_resample(train_x, train_y)

#Describe outcome of interest after SMOTE.

print("After resampling, counts of label '1': {}".format(sum(train_y == 1)))
print("After resampling, counts of label '0': {} \n".format(sum(train_y == 0)))

#define AUROC function w 95%CIs

def auroc_ci(y_test, y_probs, positive=1, n_samples=1000, alpha=0.05):
    auroc_values = []
    for i in range(n_samples):
        y_test_bs, y_probs_bs = resample(y_test, y_probs)

        auroc = roc_auc_score(y_test_bs, y_probs_bs)
        auroc_values.append(auroc)

    mean_auroc = np.mean(auroc_values)
    std_auroc = np.std(auroc_values)

    lower_ci = mean_auroc - 1.96 * std_auroc
    upper_ci = mean_auroc + 1.96 * std_auroc

    return auroc, lower_ci, upper_ci
#Define z-value for other confidence intervals.

confidence = 0.95
z_value = st.norm.ppf((1 + confidence) / 2.0)

### CATBoost Model

from optuna.integration import CatBoostPruningCallback

def objective(trial: optuna.Trial) -> float:

    param = {
        "objective": trial.suggest_categorical("objective", ["Logloss", "CrossEntropy"]),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1, log=True),
        "depth": trial.suggest_int("depth", 1, 12),
        "boosting_type": trial.suggest_categorical("boosting_type", ["Ordered", "Plain"]),
        "bootstrap_type": trial.suggest_categorical(
            "bootstrap_type", ["Bayesian", "Bernoulli", "MVS"]
        ),
        "used_ram_limit": "3gb",
        "eval_metric": "AUC",
    }

    if param["bootstrap_type"] == "Bayesian":
        param["bagging_temperature"] = trial.suggest_float("bagging_temperature", 0, 10)
    elif param["bootstrap_type"] == "Bernoulli":
        param["subsample"] = trial.suggest_float("subsample", 0.1, 1, log=True)

    gbm = CatBoostClassifier(**param)

    pruning_callback = CatBoostPruningCallback(trial, "AUC")
    gbm.fit(
        train_x,
        train_y,
        eval_set=[(valid_x, valid_y)],
        verbose=0,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )

    pruning_callback.check_pruned()

    preds = gbm.predict(valid_x)
    pred_labels = np.rint(preds)
    auc = sklearn.metrics.roc_auc_score(valid_y, pred_labels)

    return auc

if __name__ == "__main__":
    study = optuna.create_study(
        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="maximize"
    )
    study.optimize(objective, n_trials=100, timeout=600)

    print("Number of finished trials: {}".format(len(study.trials)))

    print("Best trial:")
    trial = study.best_trial

    print("  Value: {}".format(trial.value))

    print("  Params: ")
    for key, value in trial.params.items():
        print("    {}: {}".format(key, value))

    cb_params = {}

    for key, value in trial.params.items():
        cb_params[key] = value
#See CatBoost hyperparameters.

cb_params['random_seed'] = 31

print(cb_params)
#Fit CatBoost.

cb = CatBoostClassifier(**cb_params)

cb.fit(train_x, train_y)

#Predict on the validation set, get predicted probabilities for calibration, and fit calibration function.

calib_probs_cb = cb.predict_proba(valid_x)

calib_model_cb = LogisticRegression()
calib_model_cb.fit(calib_probs_cb, valid_y)
#Make predictions on the test set based on the trained CatBoost model.

preds_cb = cb.predict(x_test)

uncalibrated_probs_cb = cb.predict_proba(x_test)

probs_cb = calib_model_cb.predict_proba(uncalibrated_probs_cb)
probs_cb = probs_cb[:, 1]

#Evaluate CatBoost model on test data

cb_tn, cb_fp, cb_fn, cb_tp = confusion_matrix(y_test, preds_cb).ravel()
cb_cm = confusion_matrix(y_test, preds_cb)

cb_sensitivity = round(cb_tp/(cb_tp+cb_fn), 3)
cb_sensitivity_ci_length = z_value * np.sqrt((cb_sensitivity * (1 - cb_sensitivity)) / y_test.shape[0])
cb_sensitivity_ci_lower = round((cb_sensitivity - cb_sensitivity_ci_length), 3)
cb_sensitivity_ci_upper = round((cb_sensitivity + cb_sensitivity_ci_length), 3)
cb_sensitivity_str = str(cb_sensitivity) + ' (' + str(cb_sensitivity_ci_lower) + ' - ' + str(cb_sensitivity_ci_upper) + ')'

cb_specificity = round(cb_tn/(cb_tn+cb_fp), 3)
cb_specificity_ci_length = z_value * np.sqrt((cb_specificity * (1 - cb_specificity)) / y_test.shape[0])
cb_specificity_ci_lower = round((cb_specificity - cb_specificity_ci_length), 3)
cb_specificity_ci_upper = round((cb_specificity + cb_specificity_ci_length), 3)
cb_specificity_str = str(cb_specificity) + ' (' + str(cb_specificity_ci_lower) + ' - ' + str(cb_specificity_ci_upper) + ')'

cb_auprc = round(average_precision_score(y_test, probs_cb), 3)
cb_auprc_ci_length = z_value * np.sqrt((cb_auprc * (1 - cb_auprc)) / y_test.shape[0])
cb_auprc_ci_lower = round((cb_auprc - cb_auprc_ci_length), 3)
cb_auprc_ci_upper = round((cb_auprc + cb_auprc_ci_length), 3)
cb_auprc_str = str(cb_auprc) + ' (' + str(cb_auprc_ci_lower) + ' - ' + str(cb_auprc_ci_upper) + ')'

cb_accuracy = round(accuracy_score(y_test, preds_cb), 3)
cb_accuracy_ci_length = z_value * np.sqrt((cb_accuracy * (1 - cb_accuracy)) / y_test.shape[0])
cb_accuracy_ci_lower = round((cb_accuracy - cb_accuracy_ci_length), 3)
cb_accuracy_ci_upper = round((cb_accuracy + cb_accuracy_ci_length), 3)
cb_accuracy_str = str(cb_accuracy) + ' (' + str(cb_accuracy_ci_lower) + ' - ' + str(cb_accuracy_ci_upper) + ')'

cb_auroc, cb_auroc_ci_lower, cb_auroc_ci_upper = auroc_ci(y_test, probs_cb)
cb_auroc = round(cb_auroc, 3)
cb_auroc_ci_lower = round(cb_auroc_ci_lower, 3)
cb_auroc_ci_upper = round(cb_auroc_ci_upper, 3)
cb_auroc_str = str(cb_auroc) + ' (' + str(cb_auroc_ci_lower) + ' - ' + str(cb_auroc_ci_upper) + ')'

cb_brier = round(brier_score_loss(y_test, probs_cb), 3)
cb_brier_ci_length = z_value * np.sqrt((cb_brier * (1 - cb_brier)) / y_test.shape[0])
cb_brier_ci_lower = round((cb_brier - cb_brier_ci_length), 3)
cb_brier_ci_upper = round((cb_brier + cb_brier_ci_length), 3)
cb_brier_str = str(cb_brier) + ' (' + str(cb_brier_ci_lower) + ' - ' + str(cb_brier_ci_upper) + ')'

cb_results = [cb_sensitivity_str, cb_specificity_str, cb_auprc_str, cb_accuracy_str, cb_auroc_str, cb_brier_str]

print("Sensitivity: ", (cb_sensitivity_str))
print("Specificity: ", (cb_specificity_str))
print('AUPRC: ', (cb_auprc_str))
print('Accuracy: ', (cb_accuracy_str))
print('AUROC: ', (cb_auroc_str))
print('Brier Score: ', (cb_brier_str))


### XGBoost model

def objective(trial):

    dtrain = xgb.DMatrix(train_x, label=train_y)
    dvalid = xgb.DMatrix(valid_x, label=valid_y)

    param = {
        "seed": 31,
        "verbosity": 0,
        "objective":  trial.suggest_categorical("objective", ["binary:logistic"]),
        "eval_metric": "auc",
        "booster": trial.suggest_categorical("booster", ["gbtree"]),
        "lambda": trial.suggest_float("lambda", 1e-8, 1.0, log=True),
        "alpha": trial.suggest_float("alpha", 1e-8, 1.0, log=True),
        "max_depth" : trial.suggest_int("max_depth", 1, 9),
        "eta" : trial.suggest_float("eta", 1e-8, 1.0, log=True),
        "gamma" : trial.suggest_float("gamma", 1e-8, 1.0, log=True),
        "grow_policy" : trial.suggest_categorical("grow_policy", ["depthwise", "lossguide"])
    }


    # Add a callback for pruning.
    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, "validation-auc")

    bst = xgb.train(param, dtrain, evals=[(dvalid, "validation")], callbacks=[pruning_callback])
    preds = bst.predict(dvalid)
    pred_labels = np.rint(preds)
    auc = sklearn.metrics.roc_auc_score(valid_y, pred_labels)

    return auc


if __name__ == "__main__":
    study = optuna.create_study(
        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="maximize", sampler = TPESampler(seed=31)
    )
    study.optimize(objective, n_trials=100)
    print("Number of finished trials: {}".format(len(study.trials)))

    print("Best trial:")
    trial = study.best_trial

    print("  Value: {}".format(trial.value))

    print("  Params: ")
    for key, value in trial.params.items():
        print("    {}: {}".format(key, value))

    xgb_params = {}

    for key, value in trial.params.items():
        xgb_params[key] = value

#See XGBoost hyperparameters.

xgb_params['eval_metric'] = 'auc'
xgb_params['verbosity'] = 0
xgb_params['seed'] = 31

print(xgb_params)

#Fit XGBoost on training data

from xgboost import XGBClassifier

xgb = XGBClassifier(**xgb_params)

xgb.fit(train_x, train_y)

#Predict on the validation set, get predicted probabilities for calibration, and fit calibration function.

calib_probs_xgb = xgb.predict_proba(valid_x)

calib_model_xgb = LogisticRegression()
calib_model_xgb.fit(calib_probs_xgb, valid_y)
#Make predictions on the test set based on the trained and calibrated XGBoost model.

preds_xgb = xgb.predict(x_test)

uncalibrated_probs_xgb = xgb.predict_proba(x_test)

probs_xgb = calib_model_xgb.predict_proba(uncalibrated_probs_xgb)
probs_xgb = probs_xgb[:, 1]

#Evaluate XGBoost model on testing data

xgb_tn, xgb_fp, xgb_fn, xgb_tp = confusion_matrix(y_test, preds_xgb).ravel()
xgb_cm = confusion_matrix(y_test, preds_xgb)

xgb_sensitivity = round(xgb_tp/(xgb_tp+xgb_fn), 3)
xgb_sensitivity_ci_length = z_value * np.sqrt((xgb_sensitivity * (1 - xgb_sensitivity)) / y_test.shape[0])
xgb_sensitivity_ci_lower = round((xgb_sensitivity - xgb_sensitivity_ci_length), 3)
xgb_sensitivity_ci_upper = round((xgb_sensitivity + xgb_sensitivity_ci_length), 3)
xgb_sensitivity_str = str(xgb_sensitivity) + ' (' + str(xgb_sensitivity_ci_lower) + ' - ' + str(xgb_sensitivity_ci_upper) + ')'

xgb_specificity = round(xgb_tn/(xgb_tn+xgb_fp), 3)
xgb_specificity_ci_length = z_value * np.sqrt((xgb_specificity * (1 - xgb_specificity)) / y_test.shape[0])
xgb_specificity_ci_lower = round((xgb_specificity - xgb_specificity_ci_length), 3)
xgb_specificity_ci_upper = round((xgb_specificity + xgb_specificity_ci_length), 3)
xgb_specificity_str = str(xgb_specificity) + ' (' + str(xgb_specificity_ci_lower) + ' - ' + str(xgb_specificity_ci_upper) + ')'

xgb_auprc = round(average_precision_score(y_test, probs_xgb), 3)
xgb_auprc_ci_length = z_value * np.sqrt((xgb_auprc * (1 - xgb_auprc)) / y_test.shape[0])
xgb_auprc_ci_lower = round((xgb_auprc - xgb_auprc_ci_length), 3)
xgb_auprc_ci_upper = round((xgb_auprc + xgb_auprc_ci_length), 3)
xgb_auprc_str = str(xgb_auprc) + ' (' + str(xgb_auprc_ci_lower) + ' - ' + str(xgb_auprc_ci_upper) + ')'

xgb_accuracy = round(accuracy_score(y_test, preds_xgb), 3)
xgb_accuracy_ci_length = z_value * np.sqrt((xgb_accuracy * (1 - xgb_accuracy)) / y_test.shape[0])
xgb_accuracy_ci_lower = round((xgb_accuracy - xgb_accuracy_ci_length), 3)
xgb_accuracy_ci_upper = round((xgb_accuracy + xgb_accuracy_ci_length), 3)
xgb_accuracy_str = str(xgb_accuracy) + ' (' + str(xgb_accuracy_ci_lower) + ' - ' + str(xgb_accuracy_ci_upper) + ')'

xgb_auroc, xgb_auroc_ci_lower, xgb_auroc_ci_upper = auroc_ci(y_test, probs_xgb)
xgb_auroc = round(xgb_auroc, 3)
xgb_auroc_ci_lower = round(xgb_auroc_ci_lower, 3)
xgb_auroc_ci_upper = round(xgb_auroc_ci_upper, 3)
xgb_auroc_str = str(xgb_auroc) + ' (' + str(xgb_auroc_ci_lower) + ' - ' + str(xgb_auroc_ci_upper) + ')'

xgb_brier = round(brier_score_loss(y_test, probs_xgb), 3)
xgb_brier_ci_length = z_value * np.sqrt((xgb_brier * (1 - xgb_brier)) / y_test.shape[0])
xgb_brier_ci_lower = round((xgb_brier - xgb_brier_ci_length), 3)
xgb_brier_ci_upper = round((xgb_brier + xgb_brier_ci_length), 3)
xgb_brier_str = str(xgb_brier) + ' (' + str(xgb_brier_ci_lower) + ' - ' + str(xgb_brier_ci_upper) + ')'

xgb_results = [xgb_sensitivity_str, xgb_specificity_str, xgb_auprc_str, xgb_accuracy_str, xgb_auroc_str, xgb_brier_str]

print("Sensitivity: ", (xgb_sensitivity_str))
print("Specificity: ", (xgb_specificity_str))
print('AUPRC: ', (xgb_auprc_str))
print('Accuracy: ', (xgb_accuracy_str))
print('AUROC: ', (xgb_auroc_str))
print('Brier Score: ', (xgb_brier_str))

### LightGBM Model

def objective(trial):
    dtrain = lgb.Dataset(train_x, label=train_y)

    param = {
        "objective":  trial.suggest_categorical("objective", ["binary"]),
        "metric": "binary_logloss",
        "verbosity": -1,
        "random_state": 31,
        "boosting_type":  trial.suggest_categorical("boosting_type", ["gbdt"]),
        "lambda_l1": trial.suggest_float("lambda_l1", 1e-8, 10.0, log=True),
        "lambda_l2": trial.suggest_float("lambda_l2", 1e-8, 10.0, log=True),
        "num_leaves": trial.suggest_int("num_leaves", 2, 256),
        "feature_fraction": trial.suggest_float("feature_fraction", 0.4, 1.0),
        "bagging_fraction": trial.suggest_float("bagging_fraction", 0.4, 1.0),
        "bagging_freq": trial.suggest_int("bagging_freq", 1, 7),
        "min_child_samples": trial.suggest_int("min_child_samples", 5, 100),
    }

    gbm = lgb.train(param, dtrain)
    preds = gbm.predict(valid_x)
    pred_labels = np.rint(preds)
    auc = sklearn.metrics.roc_auc_score(valid_y, pred_labels)
    return auc


if __name__ == "__main__":
    study = optuna.create_study(direction="maximize", sampler = TPESampler(seed=31))
    study.optimize(objective, n_trials=100)

    print("Number of finished trials: {}".format(len(study.trials)))

    print("Best trial:")
    trial = study.best_trial

    print("  Value: {}".format(trial.value))

    print("  Params: ")
    for key, value in trial.params.items():
        print("    {}: {}".format(key, value))

    lgb_params = {}

    for key, value in trial.params.items():
        lgb_params[key] = value

#See LightGBM hyperparameters.

lgb_params['metric'] = 'binary_logloss'
lgb_params['verbosity'] = -1
lgb_params['random_state'] = 31

print(lgb_params)

#Fit LightGBM on training data

import lightgbm as lgb

lgb = lgb.LGBMClassifier(**lgb_params)

lgb.fit(train_x, train_y)

#Predict on the validation set, get predicted probabilities for calibration, and fit calibration function.

calib_probs_lgb = lgb.predict_proba(valid_x)

calib_model_lgb = LogisticRegression()
calib_model_lgb.fit(calib_probs_lgb, valid_y)
#Make predictions on the test set based on the trained and calibrated LightGBM model.

preds_lgb = lgb.predict(x_test)

uncalibrated_probs_lgb = lgb.predict_proba(x_test)
uncalibrated_probs_lgb = uncalibrated_probs_lgb

probs_lgb = calib_model_lgb.predict_proba(uncalibrated_probs_lgb)
probs_lgb = probs_lgb[:, 1]

#Evaluate LightGBM model on test data

lgb_tn, lgb_fp, lgb_fn, lgb_tp = confusion_matrix(y_test, preds_lgb).ravel()
lgb_cm = confusion_matrix(y_test, preds_lgb)

lgb_sensitivity = round(lgb_tp/(lgb_tp+lgb_fn), 3)
lgb_sensitivity_ci_length = z_value * np.sqrt((lgb_sensitivity * (1 - lgb_sensitivity)) / y_test.shape[0])
lgb_sensitivity_ci_lower = round((lgb_sensitivity - lgb_sensitivity_ci_length), 3)
lgb_sensitivity_ci_upper = round((lgb_sensitivity + lgb_sensitivity_ci_length), 3)
lgb_sensitivity_str = str(lgb_sensitivity) + ' (' + str(lgb_sensitivity_ci_lower) + ' - ' + str(lgb_sensitivity_ci_upper) + ')'

lgb_specificity = round(lgb_tn/(lgb_tn+lgb_fp), 3)
lgb_specificity_ci_length = z_value * np.sqrt((lgb_specificity * (1 - lgb_specificity)) / y_test.shape[0])
lgb_specificity_ci_lower = round((lgb_specificity - lgb_specificity_ci_length), 3)
lgb_specificity_ci_upper = round((lgb_specificity + lgb_specificity_ci_length), 3)
lgb_specificity_str = str(lgb_specificity) + ' (' + str(lgb_specificity_ci_lower) + ' - ' + str(lgb_specificity_ci_upper) + ')'

lgb_auprc = round(average_precision_score(y_test, probs_lgb), 3)
lgb_auprc_ci_length = z_value * np.sqrt((lgb_auprc * (1 - lgb_auprc)) / y_test.shape[0])
lgb_auprc_ci_lower = round((lgb_auprc - lgb_auprc_ci_length), 3)
lgb_auprc_ci_upper = round((lgb_auprc + lgb_auprc_ci_length), 3)
lgb_auprc_str = str(lgb_auprc) + ' (' + str(lgb_auprc_ci_lower) + ' - ' + str(lgb_auprc_ci_upper) + ')'

lgb_accuracy = round(accuracy_score(y_test, preds_lgb), 3)
lgb_accuracy_ci_length = z_value * np.sqrt((lgb_accuracy * (1 - lgb_accuracy)) / y_test.shape[0])
lgb_accuracy_ci_lower = round((lgb_accuracy - lgb_accuracy_ci_length), 3)
lgb_accuracy_ci_upper = round((lgb_accuracy + lgb_accuracy_ci_length), 3)
lgb_accuracy_str = str(lgb_accuracy) + ' (' + str(lgb_accuracy_ci_lower) + ' - ' + str(lgb_accuracy_ci_upper) + ')'

lgb_auroc, lgb_auroc_ci_lower, lgb_auroc_ci_upper = auroc_ci(y_test, probs_lgb)
lgb_auroc = round(lgb_auroc, 3)
lgb_auroc_ci_lower = round(lgb_auroc_ci_lower, 3)
lgb_auroc_ci_upper = round(lgb_auroc_ci_upper, 3)
lgb_auroc_str = str(lgb_auroc) + ' (' + str(lgb_auroc_ci_lower) + ' - ' + str(lgb_auroc_ci_upper) + ')'

lgb_brier = round(brier_score_loss(y_test, probs_lgb), 3)
lgb_brier_ci_length = z_value * np.sqrt((lgb_brier * (1 - lgb_brier)) / y_test.shape[0])
lgb_brier_ci_lower = round((lgb_brier - lgb_brier_ci_length), 3)
lgb_brier_ci_upper = round((lgb_brier + lgb_brier_ci_length), 3)
lgb_brier_str = str(lgb_brier) + ' (' + str(lgb_brier_ci_lower) + ' - ' + str(lgb_brier_ci_upper) + ')'

lgb_results = [lgb_sensitivity_str, lgb_specificity_str, lgb_auprc_str, lgb_accuracy_str, lgb_auroc_str, lgb_brier_str]

print("Sensitivity: ", (lgb_sensitivity_str))
print("Specificity: ", (lgb_specificity_str))
print('AUPRC: ', (lgb_auprc_str))
print('Accuracy: ', (lgb_accuracy_str))
print('AUROC: ', (lgb_auroc_str))
print('Brier Score: ', (lgb_brier_str))

###Random Forest Model

def objective(trial):

    param = {
        "criterion": trial.suggest_categorical("criterion", ["gini", "entropy"]),
        "random_state": 31,
        "max_features": trial.suggest_categorical("max_features", ["sqrt","log2", None]),
        "max_depth": trial.suggest_int("max_depth", 1, 100),
        "n_estimators": trial.suggest_int("n_estimators", 100, 2000, 100),
        "min_samples_leaf": trial.suggest_int("min_samples_leaf", 1, 4, 1),
        "min_samples_split": trial.suggest_int("min_samples_split", 2, 10, 1),
    }

    rf = RandomForestClassifier(**param)

    rf.fit(
        train_x,
        train_y,
    )

    preds = rf.predict(valid_x)
    pred_labels = np.rint(preds)
    auc = sklearn.metrics.roc_auc_score(valid_y, pred_labels)

    return auc


if __name__ == "__main__":
    study = optuna.create_study(direction='maximize', sampler = TPESampler(seed=31))
    study.optimize(objective, n_trials=100, timeout=600)

    print("Number of finished trials: {}".format(len(study.trials)))

    print("Best trial:")
    trial = study.best_trial

    print("  Value: {}".format(trial.value))

    print("  Params: ")
    for key, value in trial.params.items():
        print("    {}: {}".format(key, value))

    rf_params = {}

    for key, value in trial.params.items():
        rf_params[key] = value

#See Random Forest hyperparameters.

rf_params['random_state'] = 31

print(rf_params)

#Fit Random Forest on training data

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(**rf_params)
rf.fit(train_x, train_y)

#Predict on the validation set, get predicted probabilities for calibration, and fit calibration function.

calib_probs_rf = rf.predict_proba(valid_x)
calib_model_rf = LogisticRegression()
calib_model_rf.fit(calib_probs_rf, valid_y)

#Make predictions on the test set based on the trained Random Forest model.

preds_rf = rf.predict(x_test)

uncalibrated_probs_rf = rf.predict_proba(x_test)

probs_rf = calib_model_rf.predict_proba(uncalibrated_probs_rf)
probs_rf = probs_rf[:, 1]

#Evaluate Random Forest model on test data

rf_tn, rf_fp, rf_fn, rf_tp = confusion_matrix(y_test, preds_rf).ravel()
rf_cm = confusion_matrix(y_test, preds_rf)

rf_sensitivity = round(rf_tp/(rf_tp+rf_fn), 3)
rf_sensitivity_ci_length = z_value * np.sqrt((rf_sensitivity * (1 - rf_sensitivity)) / y_test.shape[0])
rf_sensitivity_ci_lower = round((rf_sensitivity - rf_sensitivity_ci_length), 3)
rf_sensitivity_ci_upper = round((rf_sensitivity + rf_sensitivity_ci_length), 3)
rf_sensitivity_str = str(rf_sensitivity) + ' (' + str(rf_sensitivity_ci_lower) + ' - ' + str(rf_sensitivity_ci_upper) + ')'

rf_specificity = round(rf_tn/(rf_tn+rf_fp), 3)
rf_specificity_ci_length = z_value * np.sqrt((rf_specificity * (1 - rf_specificity)) / y_test.shape[0])
rf_specificity_ci_lower = round((rf_specificity - rf_specificity_ci_length), 3)
rf_specificity_ci_upper = round((rf_specificity + rf_specificity_ci_length), 3)
rf_specificity_str = str(rf_specificity) + ' (' + str(rf_specificity_ci_lower) + ' - ' + str(rf_specificity_ci_upper) + ')'

rf_auprc = round(average_precision_score(y_test, probs_rf), 3)
rf_auprc_ci_length = z_value * np.sqrt((rf_auprc * (1 - rf_auprc)) / y_test.shape[0])
rf_auprc_ci_lower = round((rf_auprc - rf_auprc_ci_length), 3)
rf_auprc_ci_upper = round((rf_auprc + rf_auprc_ci_length), 3)
rf_auprc_str = str(rf_auprc) + ' (' + str(rf_auprc_ci_lower) + ' - ' + str(rf_auprc_ci_upper) + ')'

rf_accuracy = round(accuracy_score(y_test, preds_rf), 3)
rf_accuracy_ci_length = z_value * np.sqrt((rf_accuracy * (1 - rf_accuracy)) / y_test.shape[0])
rf_accuracy_ci_lower = round((rf_accuracy - rf_accuracy_ci_length), 3)
rf_accuracy_ci_upper = round((rf_accuracy + rf_accuracy_ci_length), 3)
rf_accuracy_str = str(rf_accuracy) + ' (' + str(rf_accuracy_ci_lower) + ' - ' + str(rf_accuracy_ci_upper) + ')'

rf_auroc, rf_auroc_ci_lower, rf_auroc_ci_upper = auroc_ci(y_test, probs_rf)
rf_auroc = round(rf_auroc, 3)
rf_auroc_ci_lower = round(rf_auroc_ci_lower, 3)
rf_auroc_ci_upper = round(rf_auroc_ci_upper, 3)
rf_auroc_str = str(rf_auroc) + ' (' + str(rf_auroc_ci_lower) + ' - ' + str(rf_auroc_ci_upper) + ')'

rf_brier = round(brier_score_loss(y_test, probs_rf), 3)
rf_brier_ci_length = z_value * np.sqrt((rf_brier * (1 - rf_brier)) / y_test.shape[0])
rf_brier_ci_lower = round((rf_brier - rf_brier_ci_length), 3)
rf_brier_ci_upper = round((rf_brier + rf_brier_ci_length), 3)
rf_brier_str = str(rf_brier) + ' (' + str(rf_brier_ci_lower) + ' - ' + str(rf_brier_ci_upper) + ')'

rf_results = [rf_sensitivity_str, rf_specificity_str, rf_auprc_str, rf_accuracy_str, rf_auroc_str, rf_brier_str]

print("Sensitivity: ", (rf_sensitivity_str))
print("Specificity: ", (rf_specificity_str))
print('AUPRC: ', (rf_auprc_str))
print('Accuracy: ', (rf_accuracy_str))
print('AUROC: ', (rf_auroc_str))
print('Brier Score: ', (rf_brier_str))


#Summary of results of ML models on 60- month mortality

results = {'CatBoost':cb_results, 'XGBoost':xgb_results, 'LightGBM':lgb_results, 'Random Forest':rf_results}

results = pd.DataFrame(results, columns = ['CatBoost', 'XGBoost', 'LightGBM', 'Random Forest'])
results = results.T

results.columns = ['Sensitivity (95% CI)', 'Specificity (95% CI)',  'AUPRC (95% CI)', 'Accuracy (95% CI)', 'AUROC (95% CI)', 'Brier Score (95% CI)']

results.to_csv('ncdb60msurvival_results.csv')

### Confusion matrices for ML models

cb_cm = confusion_matrix(y_test, preds_cb)

f = pyplot.figure(figsize=(8,8))

sns.heatmap(cb_cm, annot=True, fmt='d', cmap='Blues', cbar=False, annot_kws={"size": 16}, linewidths=1, linecolor='black')

labels = ['Survival', 'Mortality']
pyplot.xticks([0.5,1.5], labels, fontsize=16, fontweight='heavy')
pyplot.yticks([0.5,1.5], labels, fontsize=16, fontweight='heavy', va='center')

pyplot.xlabel('Predicted', fontsize=22, fontweight='heavy', labelpad=16)
pyplot.ylabel('Actual', fontsize=22, fontweight='heavy', labelpad=16)

pyplot.tick_params(axis="y",direction="out", pad=10)
pyplot.tick_params(axis="x",direction="out", pad=10)
pyplot.title('A', x = -0.095, y = 1.005, fontsize = 75, pad = 20)

pyplot.subplots_adjust(left=0.20, right=0.85, bottom=0.20, top=0.80)

pyplot.savefig('60m_cm_cb.png', dpi=300)
pyplot.close()

#XGB confusion matrix
xgb_cm = confusion_matrix(y_test, preds_xgb)

f = pyplot.figure(figsize=(8,8))

sns.heatmap(xgb_cm, annot=True, fmt='d', cmap='Blues', cbar=False, annot_kws={"size": 16}, linewidths=1, linecolor='black')

labels = ['Survival', 'Mortality']
pyplot.xticks([0.5,1.5], labels, fontsize=16, fontweight='heavy')
pyplot.yticks([0.5,1.5], labels, fontsize=16, fontweight='heavy', va='center')

pyplot.xlabel('Predicted', fontsize=22, fontweight='heavy', labelpad=16)
pyplot.ylabel('Actual', fontsize=22, fontweight='heavy', labelpad=16)

pyplot.tick_params(axis="y",direction="out", pad=10)
pyplot.tick_params(axis="x",direction="out", pad=10)
pyplot.title('B', x = -0.095, y = 1.005, fontsize = 75, pad = 20)

pyplot.subplots_adjust(left=0.20, right=0.85, bottom=0.20, top=0.80)

pyplot.savefig('60m_cm_xgb.png', dpi=300)
pyplot.close()

#LGBoost confusion matrix

lgb_cm = confusion_matrix(y_test, preds_lgb)

f = pyplot.figure(figsize=(8,8))

sns.heatmap(lgb_cm, annot=True, fmt='d', cmap='Blues', cbar=False, annot_kws={"size": 16}, linewidths=1, linecolor='black')

labels = ['Survival', 'Mortality']
pyplot.xticks([0.5,1.5], labels, fontsize=16, fontweight='heavy')
pyplot.yticks([0.5,1.5], labels, fontsize=16, fontweight='heavy', va='center')

pyplot.xlabel('Predicted', fontsize=22, fontweight='heavy', labelpad=16)
pyplot.ylabel('Actual', fontsize=22, fontweight='heavy', labelpad=16)

pyplot.tick_params(axis="y",direction="out", pad=10)
pyplot.tick_params(axis="x",direction="out", pad=10)
pyplot.title('C', x = -0.095, y = 1.005, fontsize = 75, pad = 20)

pyplot.subplots_adjust(left=0.20, right=0.85, bottom=0.20, top=0.80)

pyplot.savefig('60m_cm_lgb.png', dpi=300)
pyplot.close()

#Random forest confusion matrix
rf_cm = confusion_matrix(y_test, preds_rf)

f = pyplot.figure(figsize=(8,8))

sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', cbar=False, annot_kws={"size": 16}, linewidths=1, linecolor='black')

labels = ['Survival', 'Mortality']
pyplot.xticks([0.5,1.5], labels, fontsize=16, fontweight='heavy')
pyplot.yticks([0.5,1.5], labels, fontsize=16, fontweight='heavy', va='center')

pyplot.xlabel('Predicted', fontsize=22, fontweight='heavy', labelpad=16)
pyplot.ylabel('Actual', fontsize=22, fontweight='heavy', labelpad=16)

pyplot.tick_params(axis="y",direction="out", pad=10)
pyplot.tick_params(axis="x",direction="out", pad=10)
pyplot.title('D', x = -0.095, y = 1.005, fontsize = 75, pad = 20)

pyplot.subplots_adjust(left=0.20, right=0.85, bottom=0.20, top=0.80)

pyplot.savefig('60m_cm_rf.png', dpi=300)
pyplot.close()

###SHAP Plots for ML Models

import textwrap
def wrap_labels(ax, width, break_long_words=False):
    labels = []
    for label in ax.get_yticklabels():
        text = label.get_text()
        labels.append(textwrap.fill(text, width=width,
                                    break_long_words=break_long_words))
    ax.set_yticklabels(labels, rotation=0)

feature_names = x_test.columns

#Catboost

cb_explainer = shap.Explainer(cb.predict, x_test)
cb_shap_values = cb_explainer(x_test)
#Plot SHAP bar plot for CatBoost.

shap.plots.bar(cb_shap_values, max_display = 15, show=False)

fig = pyplot.gcf()
ax = pyplot.gca()
fig.set_figheight(12)
fig.set_figwidth(5)

pyplot.title('A', x = -0.5, y = 1, fontsize = 50, pad = 20)
pyplot.xlabel("Mean |SHAP Value|", fontsize =12, fontweight = 'heavy', labelpad = 8)
pyplot.tick_params(axis="y",direction="out", labelsize = 12)
pyplot.tick_params(axis="x",direction="out", labelsize = 12)

wrap_labels(ax, 30)
ax.figure

pyplot.savefig('60m_shap_cb.png', dpi=300, bbox_inches='tight')
pyplot.close()

#XGBoost

xgb_explainer = shap.Explainer(xgb.predict, x_test)
xgb_shap_values = xgb_explainer(x_test)
#Plot SHAP bar plot for XGBoost.

shap.plots.bar(xgb_shap_values, max_display = 15, show=False)

fig = pyplot.gcf()
ax = pyplot.gca()
fig.set_figheight(12)
fig.set_figwidth(5)

pyplot.title('B', x = -0.5, y = 1, fontsize = 50, pad = 20)
pyplot.xlabel("Mean |SHAP Value|", fontsize =12, fontweight = 'heavy', labelpad = 8)
pyplot.tick_params(axis="y",direction="out", labelsize = 12)
pyplot.tick_params(axis="x",direction="out", labelsize = 12)

wrap_labels(ax, 30)
ax.figure
pyplot.savefig('60m_shap_xgb.png', dpi=300, bbox_inches='tight')
pyplot.close()

#LightGBM

lgb_explainer = shap.Explainer(lgb.predict, x_test)
lgb_shap_values = lgb_explainer(x_test)
#Plot SHAP bar plot for LightGBM.

shap.plots.bar(lgb_shap_values, max_display = 15, show=False)

fig = pyplot.gcf()
ax = pyplot.gca()
fig.set_figheight(12)
fig.set_figwidth(5)

pyplot.title('C', x = -0.5, y = 1, fontsize = 50, pad = 20)
pyplot.xlabel("Mean |SHAP Value|", fontsize =12, fontweight = 'heavy', labelpad = 8)
pyplot.tick_params(axis="y",direction="out", labelsize = 12)
pyplot.tick_params(axis="x",direction="out", labelsize = 12)

wrap_labels(ax, 30)
ax.figure

pyplot.savefig('60m_shap_lgb.png', dpi=300, bbox_inches='tight')
pyplot.close()

#Random Forest

rf_explainer = shap.Explainer(rf.predict, x_test)
rf_shap_values = rf_explainer(x_test)
#Plot SHAP bar plot for Random Forest.

shap.plots.bar(rf_shap_values, max_display = 15, show=False)

fig = pyplot.gcf()
ax = pyplot.gca()
fig.set_figheight(12)
fig.set_figwidth(5)

pyplot.title('D', x = -0.5, y = 1, fontsize = 50, pad = 20)
pyplot.xlabel("Mean |SHAP Value|", fontsize =12, fontweight = 'heavy', labelpad = 8)
pyplot.tick_params(axis="y",direction="out", labelsize = 12)
pyplot.tick_params(axis="x",direction="out", labelsize = 12)

wrap_labels(ax, 30)
ax.figure

pyplot.savefig('60m_shap_rf.png', dpi=300, bbox_inches='tight')
pyplot.close()

import matplotlib.pyplot as plt
import shap

#Create 2x2 figure
fig, axs = plt.subplots(2, 2, figsize=(12, 12))

shap.plots.bar(cb_shap_values, max_display=15, show=False, ax=axs[0,0])
axs[0,0].set_title('A', fontsize=24)

shap.plots.bar(xgb_shap_values, max_display=15, show=False, ax=axs[0,1])
axs[0,1].set_title('B', fontsize=24)

shap.plots.bar(lgb_shap_values, max_display=15, show=False, ax=axs[1,0])
axs[1,0].set_title('C', fontsize=24)

shap.plots.bar(rf_shap_values, max_display=15, show=False, ax=axs[1,1])
axs[1,1].set_title('D', fontsize=24)

plt.tight_layout()
plt.savefig("combined_shap_plots.png", dpi=300, bbox_inches="tight")
plt.close()

#Save LightGBM Model

import lightgbm as lgb

lgb_model = lgb.LGBMClassifier(**lgb_params)
lgb_model.fit(train_x, train_y)
lgb_model.booster_.save_model("ncdb_lightgbm_model.txt")
