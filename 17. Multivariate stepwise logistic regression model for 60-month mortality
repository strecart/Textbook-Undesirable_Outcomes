### 17. Multivariate stepwise logistic regression model for 60-month mortality

import numpy as np
import pandas as pd
import sklearn
import sklearn.metrics
from sklearn.metrics import (
    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score,
    auc, precision_score, recall_score, accuracy_score, balanced_accuracy_score,
    brier_score_loss, confusion_matrix
)
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from scipy import stats as st
from matplotlib import pyplot as plt
from tabpfn import TabPFNClassifier
from sklearn.linear_model import LogisticRegression
from imblearn.over_sampling import SMOTE

#Bootstrapping function
def bootstrap_auroc(y_true, y_scores, n_bootstraps=1000, alpha=0.05, random_state=42):
    rng = np.random.RandomState(random_state)
    bootstrapped_scores = []
    n_samples = len(y_true)

    for i in range(n_bootstraps):
        indices = rng.choice(np.arange(n_samples), size=n_samples, replace=True)
        if len(np.unique(y_true[indices])) < 2:
            continue
        score = roc_auc_score(y_true[indices], y_scores[indices])
        bootstrapped_scores.append(score)

    bootstrapped_scores = np.array(bootstrapped_scores)
    lower_bound = np.percentile(bootstrapped_scores, 100 * alpha / 2)
    upper_bound = np.percentile(bootstrapped_scores, 100 * (1 - alpha / 2))
    return lower_bound, upper_bound


### Load data and select variables
df1 = pd.read_csv('ncdbto_preprocessed.csv')

df1['MORT_bin'] = df1['60M_MORT']

continuous_vars = ['AGE']
categorical_vars = ['RACE', 'RX_SUMM_SURG_PRIM_SITE', 'RX_HOSP_SURG_APPR_2010', 'CDCC_TOTAL_BEST',
                    'UR_CD_13', 'INSURANCE_STATUS', 'MED_INC_QUAR_2020', 'FACILITY_TYPE_CD',
                    'PRIMARY_SITE', 'HISTOLOGY', 'GRADE_MERGED', 'TNM_CLIN_T_FINAL',
                    'TNM_CLIN_N_FINAL', 'TNM_CLIN_M_FINAL', 'REGIONAL_NODES_EXAMINED',
                    'RX_SUMM_SYSTEMIC_SUR_SEQ']
candidate_vars = continuous_vars + categorical_vars

df1_clean = df1.dropna(subset=candidate_vars + ["MORT_bin"])

### Split into training, validation and testing datasets
train_data, temp_data = train_test_split(df1_clean, test_size=0.4, random_state=42)
val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)

print("Training set shape:", train_data.shape)
print("Validation set shape:", val_data.shape)
print("Testing set shape:", test_data.shape)


# Define AIC functions

def fit_model(formula, data):
    model = smf.logit(formula=formula, data=data).fit(disp=0)
    return model


def make_term(term):
    if term in categorical_vars:
        return f"C({term})"
    else:
        return term


def stepwise_selection(data, response, candidate_vars, verbose=True):
    included = []
    best_formula = f"{response} ~ 1"
    best_model = fit_model(best_formula, data)
    best_aic = best_model.aic
    if verbose:
        print(f"Start: AIC = {best_aic:.4f}")
    improved = True
    while improved:
        improved = False
        # ----- Forward step -----
        forward_aic = {}
        for term in candidate_vars:
            if term not in included:
                terms = [make_term(t) for t in included + [term]]
                formula = f"{response} ~ " + " + ".join(terms)
                try:
                    model = fit_model(formula, data)
                    forward_aic[term] = model.aic
                except np.linalg.LinAlgError as e:
                    if verbose:
                        print(f"Skipping {term} due to singular matrix error: {e}")
                    continue
        if forward_aic:
            best_forward_term = min(forward_aic, key=forward_aic.get)
            best_forward_aic = forward_aic[best_forward_term]
            if best_forward_aic < best_aic:
                included.append(best_forward_term)
                best_aic = best_forward_aic
                terms = [make_term(t) for t in included]
                best_formula = f"{response} ~ " + " + ".join(terms)
                best_model = fit_model(best_formula, data)
                improved = True
                if verbose:
                    print(f"Add {best_forward_term:30} New AIC: {best_aic:.4f}")
        # ----- Backward step -----
        if included:
            backward_aic = {}
            for term in included:
                temp_terms = included.copy()
                temp_terms.remove(term)
                if temp_terms:
                    terms = [make_term(t) for t in temp_terms]
                    formula = f"{response} ~ " + " + ".join(terms)
                else:
                    formula = f"{response} ~ 1"
                try:
                    model = fit_model(formula, data)
                    backward_aic[term] = model.aic
                except np.linalg.LinAlgError as e:
                    backward_aic[term] = float('inf')
                    if verbose:
                        print(f"Skipping removal of {term} due to singular matrix error: {e}")
            if backward_aic:
                worst_term = min(backward_aic, key=backward_aic.get)
                worst_aic = backward_aic[worst_term]
                current_terms = [make_term(t) for t in included]
                current_formula = f"{response} ~ " + " + ".join(current_terms)
                current_model = fit_model(current_formula, data)
                current_aic = current_model.aic
                if worst_aic < current_aic:
                    included.remove(worst_term)
                    best_aic = worst_aic
                    if included:
                        terms = [make_term(t) for t in included]
                        best_formula = f"{response} ~ " + " + ".join(terms)
                    else:
                        best_formula = f"{response} ~ 1"
                    best_model = fit_model(best_formula, data)
                    improved = True
                    if verbose:
                        print(f"Drop {worst_term:30} New AIC: {best_aic:.4f}")
        if not improved:
            break
    return best_model, best_formula

# Run selection and fit model on training data
final_model, final_formula = stepwise_selection(train_data, "MORT_bin", candidate_vars, verbose=True)

print("\nFinal Model Formula:")
print(final_formula)
print("\nFinal Model Summary:")
print(final_model.summary())

# Evaluate model on test data
predicted_probabilities_test = final_model.predict(test_data)
test_auroc = roc_auc_score(test_data["MORT_bin"], predicted_probabilities_test)
print(f"\nTest AUROC: {test_auroc:.3f}")

y_true = test_data["MORT_bin"].values
y_scores = np.array(predicted_probabilities_test)
ci_lower, ci_upper = bootstrap_auroc(y_true, y_scores)
print(f"Test AUROC 95% CI: {ci_lower:.3f} - {ci_upper:.3f}")
